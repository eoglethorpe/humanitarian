{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"for pulling down data from hr.info API on response level and sending to CSV\"\"\"\n",
    "#locations: https://api.hpc.tools/v1/public/location\n",
    "#clusters: https://api.hpc.tools/v1/public/global-cluster [\"code\":\"SHL\"]\n",
    "#https://api.hpc.tools/docs/v1/\n",
    "#boundary cat by looping through locations\n",
    "\n",
    "#TODO: subtract outgoing\n",
    "\n",
    "import urllib.request, json\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import grequests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "import sc_pull\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fts(object):\n",
    "    def __init__(self, sc, test=None, all_sector=False):\n",
    "        self.all_sector = all_sector\n",
    "        self.test = test\n",
    "        self.sc = sc\n",
    "\n",
    "        self.fts = self.master_pull()\n",
    "        self.merge_sc()\n",
    "        self.flows = self.extract_flows()\n",
    "\n",
    "        # remove flows col as it's quite large\n",
    "        self.fts = self.fts.drop('fts.data.flows', axis=1)\n",
    "\n",
    "    def extract_flows(self):\n",
    "        \"\"\"take all flows and add to df\"\"\"\n",
    "\n",
    "        flow_df = pd.DataFrame()\n",
    "        for v in self.fts.iterrows():\n",
    "            norm = json_normalize(v[1]['fts.data.flows'])\n",
    "            norm['fts.uid'] = v[1]['sc.uid']\n",
    "\n",
    "            if len(flow_df) == 0:\n",
    "                flow_df = norm\n",
    "            else:\n",
    "                # need to make sure we're adding t3h same c0lz\n",
    "                assert (norm.columns.all(flow_df.columns))\n",
    "                flow_df = flow_df.append(norm, ignore_index=True)\n",
    "\n",
    "        return flow_df\n",
    "\n",
    "    def master_pull(self):\n",
    "        \"\"\"group by year, country and pull down total values with optional filter by cluster\"\"\"\n",
    "        countries = json_normalize(json.loads(urllib.request.urlopen(\"https://api.hpc.tools/v1/public/location\")\n",
    "                                              .read().decode()))['data']\n",
    "\n",
    "        print('pulled countries')\n",
    "\n",
    "        self.hist = self.sc\n",
    "\n",
    "        def get_other_sects():\n",
    "            \"\"\"get comma seperated string of all other sectors if needed\"\"\"\n",
    "            r = json_normalize(json.loads(urllib.request.urlopen(\"https://api.hpc.tools/v1/public/global-cluster\")\n",
    "                                          .read().decode()))['data']\n",
    "\n",
    "            r = [v['code'] for v in r[0]]\n",
    "            r.remove('SHL')\n",
    "            return ','.join(r)\n",
    "\n",
    "            \n",
    "        self.hist['sc.fts_url'] = self.hist.apply(lambda x: \\\n",
    "                                                        'https://api.hpc.tools/v1/public/fts/flow?countryISO3={0}&year={1}' \\\n",
    "                                                        .format(x['sc.iso3'], x['sc.year']), axis=1)\n",
    "\n",
    "        if self.test:\n",
    "            hrefz = self.hist['sc.fts_url'][:self.test]\n",
    "        else:\n",
    "            hrefz = self.hist['sc.fts_url']\n",
    "\n",
    "        def exception_handler(request, exception):\n",
    "            print('Bad URL for ' + request)\n",
    "\n",
    "        resps = []\n",
    "        rs = (grequests.get(ref) for ref in hrefz)\n",
    "        resps += grequests.map(rs, exception_handler=exception_handler, size=25)\n",
    "\n",
    "        good_resps = []\n",
    "        bad_resps = []\n",
    "        for r in resps:\n",
    "            load = json.loads(r.content)\n",
    "            load['url'] = r.url\n",
    "            if r.status_code == 200:\n",
    "                good_resps.append(load)\n",
    "            else:\n",
    "                bad_resps.append(load)\n",
    "\n",
    "        print('num bad resps: ' + str(len(bad_resps)))\n",
    "        return json_normalize(good_resps).add_prefix('fts.')\n",
    "\n",
    "    def merge_sc(self):\n",
    "        self.fts = self.fts.merge(self.hist, left_on='fts.url', right_on='sc.fts_url')\n",
    "\n",
    "    def get_flow_bdown(self):\n",
    "        \"\"\"break down flows by needed columns\"\"\"\n",
    "        def hp(v):\n",
    "            if 'Plan' in json_normalize(v['destinationObjects']).values or 'Plan' in json_normalize(\n",
    "                    v['sourceObjects']).values:\n",
    "                return 'has_plan'\n",
    "            else:\n",
    "                return 'no_plan'\n",
    "\n",
    "        self.flows['plan'] = self.flows.apply(lambda x: hp(x), axis=1)\n",
    "\n",
    "        # we are skipping boundary for now as it's <<<<\n",
    "        d = self.flows[['fts.uid', 'plan', 'amountUSD']].groupby(['fts.uid', 'plan'], as_index=False).aggregate(\n",
    "            {'amountUSD': 'sum'})\n",
    "        piv = pd.pivot_table(d, index='fts.uid', values='amountUSD', aggfunc=np.sum, columns=[d['plan']] \\\n",
    "                             , fill_value=0).reset_index()\n",
    "        piv['total'] = piv['has_plan'] + piv['no_plan']\n",
    "        return piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://api.hpc.tools/v1/public/fts/flow?countryISO3=PAK&year=2005&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=COD&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=IDN&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=LBN&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=LBR&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=PHL&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=SOM&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=UGA&year=2006&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=BGD&year=2007&globalClusterCode=',\n",
       "       'https://api.hpc.tools/v1/public/fts/flow?countryISO3=CAF&year=2007&globalClusterCode='],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['fts.url'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 227 SC entries\n",
      "pulled countries\n",
      "num bad resps: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ewanog/Documents/work/code/venv/p3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "import sc_pull\n",
    "sc = sc_pull.pull()\n",
    "s_f = fts(test=10, sc=sc)\n",
    "# all_f = fts(test=10, sc=sc, all_sector=True)\n",
    "\n",
    "# # print(s_f.get_flow_bdown().add_prefix('sc_')) #351513201 PAK2005 sc\n",
    "# print(all_f.get_flow_bdown().add_prefix('sc_')) #351513201 PAK2005 all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_b = all_f.get_flow_bdown().add_prefix('non_sc_')\n",
    "sf_b = s_f.get_flow_bdown().add_prefix('sc_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all: 197823342\n",
    "\n",
    "#sf_b: 115218527 197823342\n",
    "\n",
    "all_b['y'] = all_b.apply(lambda x : x['non_sc_fts.uid'][3:], axis = 1)\n",
    "\n",
    "all_b = all_b.sort_values(by = 'y')\n",
    "all_b.to_csv('../d0cz/all_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>plan</th>\n",
       "      <th>non_sc_fts.uid</th>\n",
       "      <th>non_sc_has_plan</th>\n",
       "      <th>non_sc_no_plan</th>\n",
       "      <th>non_sc_total</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BGD2007</td>\n",
       "      <td>0</td>\n",
       "      <td>295616727</td>\n",
       "      <td>295616727</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAF2007</td>\n",
       "      <td>86113040</td>\n",
       "      <td>11204447</td>\n",
       "      <td>97317487</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COD2006</td>\n",
       "      <td>167160755</td>\n",
       "      <td>18748848</td>\n",
       "      <td>185909603</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IDN2006</td>\n",
       "      <td>16143593</td>\n",
       "      <td>63193776</td>\n",
       "      <td>79337369</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LBN2006</td>\n",
       "      <td>79834450</td>\n",
       "      <td>105361386</td>\n",
       "      <td>185195836</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LBR2006</td>\n",
       "      <td>79995073</td>\n",
       "      <td>43819252</td>\n",
       "      <td>123814325</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PAK2005</td>\n",
       "      <td>57609250</td>\n",
       "      <td>140214092</td>\n",
       "      <td>197823342</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PHL2006</td>\n",
       "      <td>6196188</td>\n",
       "      <td>22144179</td>\n",
       "      <td>28340367</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SOM2006</td>\n",
       "      <td>178499360</td>\n",
       "      <td>57900511</td>\n",
       "      <td>236399871</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UGA2006</td>\n",
       "      <td>203993866</td>\n",
       "      <td>44041589</td>\n",
       "      <td>248035455</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "plan non_sc_fts.uid  non_sc_has_plan  non_sc_no_plan  non_sc_total     y\n",
       "0           BGD2007                0       295616727     295616727  2007\n",
       "1           CAF2007         86113040        11204447      97317487  2007\n",
       "2           COD2006        167160755        18748848     185909603  2006\n",
       "3           IDN2006         16143593        63193776      79337369  2006\n",
       "4           LBN2006         79834450       105361386     185195836  2006\n",
       "5           LBR2006         79995073        43819252     123814325  2006\n",
       "6           PAK2005         57609250       140214092     197823342  2005\n",
       "7           PHL2006          6196188        22144179      28340367  2006\n",
       "8           SOM2006        178499360        57900511     236399871  2006\n",
       "9           UGA2006        203993866        44041589     248035455  2006"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-581-41cc2d97e67c>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-581-41cc2d97e67c>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    g = fs[[‘amountUSD’,‘boundary', 'status’]].groupby(‘boundary', 'status’).sum()\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#in progress\n",
    "\n",
    "gran_cols = ['incoming_commitment', 'incoming_paid', 'incoming_pledge', 'outgoing_commitment', \n",
    "            'outgoing_paid', 'outgoing_pledge', 'internal_commitment', 'internal_paid', 'internal_pledge']\n",
    "\n",
    "\n",
    "def check_sum(uid):\n",
    "    \"\"\"check sums are good\"\"\"\n",
    "    sm = sum([ft[ft['sc.uid'] == uid][g] for g in gran_cols if ft[ft['sc.uid'] == uid][g]][0])\n",
    "\n",
    "def pop_flows():\n",
    "    g = fs.groupby(by = ['sc.uid', 'boundary', 'status']).agg({'amountUSD' : 'sum'})\n",
    "    g = fs[[‘amountUSD’,‘boundary', 'status’]].groupby(‘boundary', 'status’).sum()\n",
    "    g = g.reset_index()\n",
    "        \n",
    "    for c in gran_cols:\n",
    "        ft[c] = None\n",
    "\n",
    "    for v in g.iterrows():\n",
    "        var_nm = v[1]['boundary'] + '_' + v[1]['status']\n",
    "        \n",
    "        if len(ft.loc[ft['sc.uid'] == v[1]['sc.uid']]) != 1:\n",
    "            raise Exception('bad UID')\n",
    "        \n",
    "        if v[1]['sc.uid'] not in list(ft['sc.uid']):\n",
    "            print(v[1]['sc.uid'])\n",
    "        \n",
    "        ft.loc[ft['sc.uid'] == v[1]['sc.uid'], var_nm] = v[1]['amountUSD']\n",
    "        \n",
    "        #check all sums are good\n",
    "#         check_sum(v[1]['sc.uid'])\n",
    "        \n",
    "#     df[col_list].sum(axis=1)\n",
    "    \n",
    "pop_flows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "def get_bound_types(x):\n",
    "    \"\"\"see what count boundary transactions we have\n",
    "        {'incoming': 4889, 'internal': 10, 'outgoing': 23}\n",
    "    \"\"\"\n",
    "    norm = json_normalize(x['fts.data.flows'])\n",
    "    \n",
    "    if 'boundary' in norm:\n",
    "        for v in norm['boundary']:\n",
    "            if v == 'outgoing':\n",
    "                print(x)\n",
    "                raise('sfd')\n",
    "\n",
    "            if v not in bounds:\n",
    "                bounds[v] = 1\n",
    "            else:\n",
    "                bounds[v] += 1\n",
    "    \n",
    "j.apply(lambda x : get_bound_types(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test to see if we're figuring out outgoing correctly\n",
    "ent = j[j['fts.data.incoming.fundingTotal'] == 1597544]\n",
    "\n",
    "#regular:\n",
    "json_normalize(ent['fts.data.flows'][9]).groupby(by = 'status')['amountUSD'].sum()\n",
    "# commitment    1130923\n",
    "# paid           616621\n",
    "\n",
    "#subtract:\n",
    "r = json_normalize(ent['fts.data.flows'][9]).groupby(by = ['status', 'boundary'])['amountUSD'].sum()\n",
    "\n",
    "list(set(r.index.get_level_values('status')))\n",
    "\n",
    "r.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
